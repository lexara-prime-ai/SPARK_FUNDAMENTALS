{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark(Spark's python wrapper)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|Index|Organization Id|                Name|             Website|             Country|         Description|Founded|            Industry|Number of employees|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|    1|E84A904909dF528|          Liu-Hoover|http://www.day-ha...|      Western Sahara|Ergonomic zero ad...|   1980|   Online Publishing|               6851|\n",
      "|    2|AAC4f9aBF86EAeF|       Orr-Armstrong|https://www.chapm...|             Algeria|Ergonomic radical...|   1970|     Import / Export|               7994|\n",
      "|    3|ad2eb3C8C24DB87|           Gill-Lamb|     http://lin.com/|       Cote d'Ivoire|Programmable inte...|   2005|   Apparel / Fashion|               5105|\n",
      "|    4|D76BB12E5eE165B|         Bauer-Weiss|https://gillespie...|United States of ...|Synergistic maxim...|   2015|               Dairy|               9069|\n",
      "|    5|2F31EddF2Db9aAE|         Love-Palmer| https://kramer.com/|             Denmark|Optimized optimiz...|   2010|Management Consul...|               6991|\n",
      "|    6|6774DC1dB00BD11|Farmer, Edwards a...|http://wolfe-boyd...|      Norfolk Island|Virtual leadinged...|   2003|  Mental Health Care|               3503|\n",
      "|    7|116B5cD4eE1fAAc|Bass, Hester and ...|https://meza-smit...|          Uzbekistan|Multi-tiered syst...|   1994|   Computer Hardware|               2762|\n",
      "|    8|AB2eA15d98b6BD4|Strickland, Gray ...|   http://kerr.info/|              Israel|Team-oriented fre...|   1987|     Performing Arts|               7020|\n",
      "|    9|0c6D831e8DceCfE|Sparks, Decker an...|https://www.howe....|              Israel|Down-sized conten...|   1977|Marketing / Adver...|               2709|\n",
      "|   10|9ABE0c8aee135d6|Osborn, Ford and ...|http://www.mcdona...|Syrian Arab Republic|Optional coherent...|   1990|Investment Bankin...|               5731|\n",
      "|   11|C494aBe0CDB9c70|        Gonzales Inc|    http://lara.org/|              Angola|Enterprise-wide a...|   2019|Capital Markets /...|               2252|\n",
      "|   12|9FBF69aa2D9AAF1|Ballard, Goodman ...|http://berger-che...|                Iran|Multi-layered zer...|   2019|Logistics / Procu...|               6165|\n",
      "|   13|4EB9d3E5cF79b91|Bernard, Payne an...|http://williamson...|         Afghanistan|Polarized dynamic...|   1990|      Transportation|                730|\n",
      "|   14|e5CA6529DCD4030| Mcpherson-Blanchard|http://www.ward.com/|            Bulgaria|Integrated didact...|   1972|     Law Enforcement|               9890|\n",
      "|   15|b0BBeBB36bAbb35|Koch, Gomez and Hays|http://ewing-rosa...|French Southern T...|Operative nationa...|   1997|  Financial Services|               8497|\n",
      "|   16|CDa7Ec784D5A8Bd|Meza, Ramirez and...|  https://hardy.biz/|         Timor-Leste|Customer-focused ...|   2012|Public Relations ...|               5205|\n",
      "|   17|b7cBE6A9E79A56F|Morales, Hinton a...| https://hudson.com/|              Serbia|Seamless global u...|   2014|Museums / Institu...|               6706|\n",
      "|   18|e5cab57FFF6f4bd|       Abbott-Arroyo| http://delgado.net/|             Hungary|Visionary holisti...|   1994|Architecture / Pl...|                117|\n",
      "|   19|3851f381Ea54d70|       Trevino-Foley| https://baxter.com/|              Canada|Enterprise-wide 6...|   1973|      Wine / Spirits|               6765|\n",
      "|   20|2FaDD3Ac0fB4052|Bowman, Walters a...|https://www.drake...|               Yemen|Exclusive client-...|   1988|         Think Tanks|               5926|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file path\n",
    "file_path = \"./data/people.csv\"\n",
    "\n",
    "# Load data from a csv file\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Preview data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define table name\n",
    "table_name = \"organizations\"\n",
    "\n",
    "# Create an SQL table \n",
    "df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|Index|Organization Id|                Name|             Website|             Country|         Description|Founded|            Industry|Number of employees|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|    1|E84A904909dF528|          Liu-Hoover|http://www.day-ha...|      Western Sahara|Ergonomic zero ad...|   1980|   Online Publishing|               6851|\n",
      "|    2|AAC4f9aBF86EAeF|       Orr-Armstrong|https://www.chapm...|             Algeria|Ergonomic radical...|   1970|     Import / Export|               7994|\n",
      "|    3|ad2eb3C8C24DB87|           Gill-Lamb|     http://lin.com/|       Cote d'Ivoire|Programmable inte...|   2005|   Apparel / Fashion|               5105|\n",
      "|    4|D76BB12E5eE165B|         Bauer-Weiss|https://gillespie...|United States of ...|Synergistic maxim...|   2015|               Dairy|               9069|\n",
      "|    5|2F31EddF2Db9aAE|         Love-Palmer| https://kramer.com/|             Denmark|Optimized optimiz...|   2010|Management Consul...|               6991|\n",
      "|    6|6774DC1dB00BD11|Farmer, Edwards a...|http://wolfe-boyd...|      Norfolk Island|Virtual leadinged...|   2003|  Mental Health Care|               3503|\n",
      "|    7|116B5cD4eE1fAAc|Bass, Hester and ...|https://meza-smit...|          Uzbekistan|Multi-tiered syst...|   1994|   Computer Hardware|               2762|\n",
      "|    8|AB2eA15d98b6BD4|Strickland, Gray ...|   http://kerr.info/|              Israel|Team-oriented fre...|   1987|     Performing Arts|               7020|\n",
      "|    9|0c6D831e8DceCfE|Sparks, Decker an...|https://www.howe....|              Israel|Down-sized conten...|   1977|Marketing / Adver...|               2709|\n",
      "|   10|9ABE0c8aee135d6|Osborn, Ford and ...|http://www.mcdona...|Syrian Arab Republic|Optional coherent...|   1990|Investment Bankin...|               5731|\n",
      "|   11|C494aBe0CDB9c70|        Gonzales Inc|    http://lara.org/|              Angola|Enterprise-wide a...|   2019|Capital Markets /...|               2252|\n",
      "|   12|9FBF69aa2D9AAF1|Ballard, Goodman ...|http://berger-che...|                Iran|Multi-layered zer...|   2019|Logistics / Procu...|               6165|\n",
      "|   13|4EB9d3E5cF79b91|Bernard, Payne an...|http://williamson...|         Afghanistan|Polarized dynamic...|   1990|      Transportation|                730|\n",
      "|   14|e5CA6529DCD4030| Mcpherson-Blanchard|http://www.ward.com/|            Bulgaria|Integrated didact...|   1972|     Law Enforcement|               9890|\n",
      "|   15|b0BBeBB36bAbb35|Koch, Gomez and Hays|http://ewing-rosa...|French Southern T...|Operative nationa...|   1997|  Financial Services|               8497|\n",
      "|   16|CDa7Ec784D5A8Bd|Meza, Ramirez and...|  https://hardy.biz/|         Timor-Leste|Customer-focused ...|   2012|Public Relations ...|               5205|\n",
      "|   17|b7cBE6A9E79A56F|Morales, Hinton a...| https://hudson.com/|              Serbia|Seamless global u...|   2014|Museums / Institu...|               6706|\n",
      "|   18|e5cab57FFF6f4bd|       Abbott-Arroyo| http://delgado.net/|             Hungary|Visionary holisti...|   1994|Architecture / Pl...|                117|\n",
      "|   19|3851f381Ea54d70|       Trevino-Foley| https://baxter.com/|              Canada|Enterprise-wide 6...|   1973|      Wine / Spirits|               6765|\n",
      "|   20|2FaDD3Ac0fB4052|Bowman, Walters a...|https://www.drake...|               Yemen|Exclusive client-...|   1988|         Think Tanks|               5926|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define query\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "# Query the table\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           col_name|\n",
      "+-------------------+\n",
      "|              Index|\n",
      "|    Organization Id|\n",
      "|               Name|\n",
      "|            Website|\n",
      "|            Country|\n",
      "|        Description|\n",
      "|            Founded|\n",
      "|           Industry|\n",
      "|Number of employees|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index', 'Organization Id', 'Name', 'Website', 'Country', 'Description', 'Founded', 'Industry', 'Number of employees']\n"
     ]
    }
   ],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "\n",
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|              Index|   string|   null|\n",
      "|    Organization Id|   string|   null|\n",
      "|               Name|   string|   null|\n",
      "|            Website|   string|   null|\n",
      "|            Country|   string|   null|\n",
      "|        Description|   string|   null|\n",
      "|            Founded|   string|   null|\n",
      "|           Industry|   string|   null|\n",
      "|Number of employees|   string|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Function SQL\n",
    "* OVER and ORDERBY clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train schedule dataset\n",
    "df = spark.read.csv(\"./data/train_schedule.csv\", header=True)\n",
    "df.createOrReplaceTempView(\"sched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+\n",
      "|train_id|             station|    time|\n",
      "+--------+--------------------+--------+\n",
      "|     110|      Sydney Central|01:05 PM|\n",
      "|     104|        Gare du Nord|01:20 PM|\n",
      "|     107|       Madrid Atocha|01:50 PM|\n",
      "|     115|   Los Angeles Union|01:50 PM|\n",
      "|     102|         Kings Cross|02:00 PM|\n",
      "|     113|        Cairo Ramses|02:35 PM|\n",
      "|     110|   Singapore Central|03:20 PM|\n",
      "|     105|       Tokyo Station|03:35 PM|\n",
      "|     108| Berlin Hauptbahnhof|04:05 PM|\n",
      "|     102|    Shinjuku Station|04:20 PM|\n",
      "|     113| Buenos Aires Retiro|04:50 PM|\n",
      "|     111|Mexico City Terminal|05:35 PM|\n",
      "|     105|   Hong Kong Station|05:50 PM|\n",
      "|     108|  Amsterdam Centraal|06:20 PM|\n",
      "|     103|            Waterloo|06:35 PM|\n",
      "|     106|    Liverpool Street|07:05 AM|\n",
      "|     114|Toronto Union Sta...|07:05 AM|\n",
      "|     111|     Moscow Kazansky|07:50 AM|\n",
      "|     101|       Grand Central|08:15 AM|\n",
      "|     109|        Rome Termini|08:35 AM|\n",
      "+--------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sched ORDER BY time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/24 14:01:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/24 14:01:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/24 14:01:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/24 14:01:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/24 14:01:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+--------+-------------+--------+---------+\n",
      "|train_id|      station|    time|time_next|\n",
      "+--------+-------------+--------+---------+\n",
      "|     101|Grand Central|08:15 AM| 10:30 AM|\n",
      "|     101| Penn Station|10:30 AM| 12:45 PM|\n",
      "|     101|Union Station|12:45 PM|     null|\n",
      "+--------+-------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "WHERE train_id=101\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+---------+\n",
      "|train_id|            station|    time|time_next|\n",
      "+--------+-------------------+--------+---------+\n",
      "|     101|      Grand Central|08:15 AM| 10:30 AM|\n",
      "|     101|       Penn Station|10:30 AM| 12:45 PM|\n",
      "|     101|      Union Station|12:45 PM|     null|\n",
      "|     102|        Kings Cross|02:00 PM| 04:20 PM|\n",
      "|     102|   Shinjuku Station|04:20 PM|     null|\n",
      "|     103|           Waterloo|06:35 PM| 08:50 AM|\n",
      "|     103|   Victoria Station|08:50 AM|     null|\n",
      "|     104|       Gare du Nord|01:20 PM| 11:05 AM|\n",
      "|     104|       King's Cross|11:05 AM|     null|\n",
      "|     105|      Tokyo Station|03:35 PM| 05:50 PM|\n",
      "|     105|  Hong Kong Station|05:50 PM|     null|\n",
      "|     106|   Liverpool Street|07:05 AM| 09:20 AM|\n",
      "|     106|      Beijing South|09:20 AM|     null|\n",
      "|     107|      Madrid Atocha|01:50 PM| 11:35 AM|\n",
      "|     107|      Seoul Station|11:35 AM|     null|\n",
      "|     108|Berlin Hauptbahnhof|04:05 PM| 06:20 PM|\n",
      "|     108| Amsterdam Centraal|06:20 PM|     null|\n",
      "|     109|       Rome Termini|08:35 AM| 10:50 AM|\n",
      "|     109|Zurich Hauptbahnhof|10:50 AM|     null|\n",
      "|     110|     Sydney Central|01:05 PM| 03:20 PM|\n",
      "+--------+-------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the PARTITION BY clause in order to improve performance\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following are example queries\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Updated query -> Query did not include PARTITION BY clause as well as bad_row number\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "# Give the number of the bad row as an integer\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause, SQL keywords in upper case\n",
    "clause = 'PARTITION BY train_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_id', 'station', 'time']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ways to select 2 columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+\n",
      "|train_id|         station|    time|\n",
      "+--------+----------------+--------+\n",
      "|     101|   Grand Central|08:15 AM|\n",
      "|     101|    Penn Station|10:30 AM|\n",
      "|     101|   Union Station|12:45 PM|\n",
      "|     102|     Kings Cross|02:00 PM|\n",
      "|     102|Shinjuku Station|04:20 PM|\n",
      "+--------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|train_id|         station|\n",
      "+--------+----------------+\n",
      "|     101|   Grand Central|\n",
      "|     101|    Penn Station|\n",
      "|     101|   Union Station|\n",
      "|     102|     Kings Cross|\n",
      "|     102|Shinjuku Station|\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show only 5 records and 2 columns\n",
    "df.select('train_id', 'station').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|train_id|         station|\n",
      "+--------+----------------+\n",
      "|     101|   Grand Central|\n",
      "|     101|    Penn Station|\n",
      "|     101|   Union Station|\n",
      "|     102|     Kings Cross|\n",
      "|     102|Shinjuku Station|\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The same can be achieved using <dot > notation\n",
    "df.select(df.train_id, df.station).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The <col> function can also be imported\n",
    "# This enables passing in column names as strings\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|train_id|         station|\n",
      "+--------+----------------+\n",
      "|     101|   Grand Central|\n",
      "|     101|    Penn Station|\n",
      "|     101|   Union Station|\n",
      "|     102|     Kings Cross|\n",
      "|     102|Shinjuku Station|\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('train_id'), col('station')).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
