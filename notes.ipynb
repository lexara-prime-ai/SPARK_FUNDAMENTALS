{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark(Spark's python wrapper)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define file path\n",
    "file_path = \"./data/people.csv\"\n",
    "\n",
    "# Load data from a csv file\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Preview data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define table name\n",
    "table_name = \"organizations\"\n",
    "\n",
    "# Create an SQL table \n",
    "df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "# Query the table\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "\n",
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Function SQL\n",
    "* OVER and ORDERBY clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train schedule dataset\n",
    "df = spark.read.csv(\"./data/train_schedule.csv\", header=True)\n",
    "df.createOrReplaceTempView(\"sched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sched ORDER BY time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "WHERE train_id=101\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the PARTITION BY clause in order to improve performance\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following are example queries\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Updated query -> Query did not include PARTITION BY clause as well as bad_row number\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "# Give the number of the bad row as an integer\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause, SQL keywords in upper case\n",
    "clause = 'PARTITION BY train_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ways to select 2 columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show only 5 records and 2 columns\n",
    "df.select('train_id', 'station').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The same can be achieved using <dot > notation\n",
    "df.select(df.train_id, df.station).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The <col> function can also be imported\n",
    "# This enables passing in column names as strings\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select(col('train_id'), col('station')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using the <withColumnRenamed> function\n",
    "df.select('train_id', 'station').withColumnRenamed('train_id', 'train').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note -> Avoid using all 3 conventions at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "table_name = 'sched'\n",
    "target_col = 'train_id'\n",
    "sup_col = 'station'\n",
    "\n",
    "spark.sql(f'SELECT {target_col} AS train, {sup_col} FROM {table_name} LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using dot notation to achieve the same results\n",
    "df.select(col('train_id').alias('train'), 'station').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using Window Functions to achieve the same results\n",
    "\n",
    "# The following query adds a number to each stop on a train line -- in a new column called id \n",
    "query = \"\"\"\n",
    "SELECT *,\n",
    "ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\n",
    "FROM sched\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using Window Functions to achieve the same results with dot notation\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df.withColumn(\"id\", row_number().over(Window.partitionBy('train_id').orderBy('time'))).show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Consider the following examples\n",
    "# Give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM sched GROUP BY train_id').show()\n",
    "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM sched GROUP BY train_id').show()\n",
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregating the same column twice\n",
    "\n",
    "# There are cases where dot notation can be more cumbersome than SQL. \n",
    "# This sample code calculates the first and last times for each train line. \n",
    "# The following code does this using dot notation.\n",
    "\n",
    "from pyspark.sql.functions import min, max, col\n",
    "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\n",
    "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "dot_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregating the same column twice in SQL\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT train_id, MIN(time) as start, MAX(time) as end FROM sched GROUP BY train_id\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
