{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import pyspark(Spark's python wrapper)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|Index|Organization Id|                Name|             Website|             Country|         Description|Founded|            Industry|Number of employees|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|    1|E84A904909dF528|          Liu-Hoover|http://www.day-ha...|      Western Sahara|Ergonomic zero ad...|   1980|   Online Publishing|               6851|\n",
      "|    2|AAC4f9aBF86EAeF|       Orr-Armstrong|https://www.chapm...|             Algeria|Ergonomic radical...|   1970|     Import / Export|               7994|\n",
      "|    3|ad2eb3C8C24DB87|           Gill-Lamb|     http://lin.com/|       Cote d'Ivoire|Programmable inte...|   2005|   Apparel / Fashion|               5105|\n",
      "|    4|D76BB12E5eE165B|         Bauer-Weiss|https://gillespie...|United States of ...|Synergistic maxim...|   2015|               Dairy|               9069|\n",
      "|    5|2F31EddF2Db9aAE|         Love-Palmer| https://kramer.com/|             Denmark|Optimized optimiz...|   2010|Management Consul...|               6991|\n",
      "|    6|6774DC1dB00BD11|Farmer, Edwards a...|http://wolfe-boyd...|      Norfolk Island|Virtual leadinged...|   2003|  Mental Health Care|               3503|\n",
      "|    7|116B5cD4eE1fAAc|Bass, Hester and ...|https://meza-smit...|          Uzbekistan|Multi-tiered syst...|   1994|   Computer Hardware|               2762|\n",
      "|    8|AB2eA15d98b6BD4|Strickland, Gray ...|   http://kerr.info/|              Israel|Team-oriented fre...|   1987|     Performing Arts|               7020|\n",
      "|    9|0c6D831e8DceCfE|Sparks, Decker an...|https://www.howe....|              Israel|Down-sized conten...|   1977|Marketing / Adver...|               2709|\n",
      "|   10|9ABE0c8aee135d6|Osborn, Ford and ...|http://www.mcdona...|Syrian Arab Republic|Optional coherent...|   1990|Investment Bankin...|               5731|\n",
      "|   11|C494aBe0CDB9c70|        Gonzales Inc|    http://lara.org/|              Angola|Enterprise-wide a...|   2019|Capital Markets /...|               2252|\n",
      "|   12|9FBF69aa2D9AAF1|Ballard, Goodman ...|http://berger-che...|                Iran|Multi-layered zer...|   2019|Logistics / Procu...|               6165|\n",
      "|   13|4EB9d3E5cF79b91|Bernard, Payne an...|http://williamson...|         Afghanistan|Polarized dynamic...|   1990|      Transportation|                730|\n",
      "|   14|e5CA6529DCD4030| Mcpherson-Blanchard|http://www.ward.com/|            Bulgaria|Integrated didact...|   1972|     Law Enforcement|               9890|\n",
      "|   15|b0BBeBB36bAbb35|Koch, Gomez and Hays|http://ewing-rosa...|French Southern T...|Operative nationa...|   1997|  Financial Services|               8497|\n",
      "|   16|CDa7Ec784D5A8Bd|Meza, Ramirez and...|  https://hardy.biz/|         Timor-Leste|Customer-focused ...|   2012|Public Relations ...|               5205|\n",
      "|   17|b7cBE6A9E79A56F|Morales, Hinton a...| https://hudson.com/|              Serbia|Seamless global u...|   2014|Museums / Institu...|               6706|\n",
      "|   18|e5cab57FFF6f4bd|       Abbott-Arroyo| http://delgado.net/|             Hungary|Visionary holisti...|   1994|Architecture / Pl...|                117|\n",
      "|   19|3851f381Ea54d70|       Trevino-Foley| https://baxter.com/|              Canada|Enterprise-wide 6...|   1973|      Wine / Spirits|               6765|\n",
      "|   20|2FaDD3Ac0fB4052|Bowman, Walters a...|https://www.drake...|               Yemen|Exclusive client-...|   1988|         Think Tanks|               5926|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define file path\n",
    "file_path = \"./data/people.csv\"\n",
    "\n",
    "# Load data from a csv file\n",
    "df = spark.read.csv(file_path, header=True)\n",
    "\n",
    "# Preview data\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define table name\n",
    "table_name = \"organizations\"\n",
    "\n",
    "# Create an SQL table \n",
    "df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|Index|Organization Id|                Name|             Website|             Country|         Description|Founded|            Industry|Number of employees|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "|    1|E84A904909dF528|          Liu-Hoover|http://www.day-ha...|      Western Sahara|Ergonomic zero ad...|   1980|   Online Publishing|               6851|\n",
      "|    2|AAC4f9aBF86EAeF|       Orr-Armstrong|https://www.chapm...|             Algeria|Ergonomic radical...|   1970|     Import / Export|               7994|\n",
      "|    3|ad2eb3C8C24DB87|           Gill-Lamb|     http://lin.com/|       Cote d'Ivoire|Programmable inte...|   2005|   Apparel / Fashion|               5105|\n",
      "|    4|D76BB12E5eE165B|         Bauer-Weiss|https://gillespie...|United States of ...|Synergistic maxim...|   2015|               Dairy|               9069|\n",
      "|    5|2F31EddF2Db9aAE|         Love-Palmer| https://kramer.com/|             Denmark|Optimized optimiz...|   2010|Management Consul...|               6991|\n",
      "|    6|6774DC1dB00BD11|Farmer, Edwards a...|http://wolfe-boyd...|      Norfolk Island|Virtual leadinged...|   2003|  Mental Health Care|               3503|\n",
      "|    7|116B5cD4eE1fAAc|Bass, Hester and ...|https://meza-smit...|          Uzbekistan|Multi-tiered syst...|   1994|   Computer Hardware|               2762|\n",
      "|    8|AB2eA15d98b6BD4|Strickland, Gray ...|   http://kerr.info/|              Israel|Team-oriented fre...|   1987|     Performing Arts|               7020|\n",
      "|    9|0c6D831e8DceCfE|Sparks, Decker an...|https://www.howe....|              Israel|Down-sized conten...|   1977|Marketing / Adver...|               2709|\n",
      "|   10|9ABE0c8aee135d6|Osborn, Ford and ...|http://www.mcdona...|Syrian Arab Republic|Optional coherent...|   1990|Investment Bankin...|               5731|\n",
      "|   11|C494aBe0CDB9c70|        Gonzales Inc|    http://lara.org/|              Angola|Enterprise-wide a...|   2019|Capital Markets /...|               2252|\n",
      "|   12|9FBF69aa2D9AAF1|Ballard, Goodman ...|http://berger-che...|                Iran|Multi-layered zer...|   2019|Logistics / Procu...|               6165|\n",
      "|   13|4EB9d3E5cF79b91|Bernard, Payne an...|http://williamson...|         Afghanistan|Polarized dynamic...|   1990|      Transportation|                730|\n",
      "|   14|e5CA6529DCD4030| Mcpherson-Blanchard|http://www.ward.com/|            Bulgaria|Integrated didact...|   1972|     Law Enforcement|               9890|\n",
      "|   15|b0BBeBB36bAbb35|Koch, Gomez and Hays|http://ewing-rosa...|French Southern T...|Operative nationa...|   1997|  Financial Services|               8497|\n",
      "|   16|CDa7Ec784D5A8Bd|Meza, Ramirez and...|  https://hardy.biz/|         Timor-Leste|Customer-focused ...|   2012|Public Relations ...|               5205|\n",
      "|   17|b7cBE6A9E79A56F|Morales, Hinton a...| https://hudson.com/|              Serbia|Seamless global u...|   2014|Museums / Institu...|               6706|\n",
      "|   18|e5cab57FFF6f4bd|       Abbott-Arroyo| http://delgado.net/|             Hungary|Visionary holisti...|   1994|Architecture / Pl...|                117|\n",
      "|   19|3851f381Ea54d70|       Trevino-Foley| https://baxter.com/|              Canada|Enterprise-wide 6...|   1973|      Wine / Spirits|               6765|\n",
      "|   20|2FaDD3Ac0fB4052|Bowman, Walters a...|https://www.drake...|               Yemen|Exclusive client-...|   1988|         Think Tanks|               5926|\n",
      "+-----+---------------+--------------------+--------------------+--------------------+--------------------+-------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define query\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "# Query the table\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           col_name|\n",
      "+-------------------+\n",
      "|              Index|\n",
      "|    Organization Id|\n",
      "|               Name|\n",
      "|            Website|\n",
      "|            Country|\n",
      "|        Description|\n",
      "|            Founded|\n",
      "|           Industry|\n",
      "|Number of employees|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Index', 'Organization Id', 'Name', 'Website', 'Country', 'Description', 'Founded', 'Industry', 'Number of employees']\n"
     ]
    }
   ],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "\n",
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+-------+\n",
      "|           col_name|data_type|comment|\n",
      "+-------------------+---------+-------+\n",
      "|              Index|   string|   null|\n",
      "|    Organization Id|   string|   null|\n",
      "|               Name|   string|   null|\n",
      "|            Website|   string|   null|\n",
      "|            Country|   string|   null|\n",
      "|        Description|   string|   null|\n",
      "|            Founded|   string|   null|\n",
      "|           Industry|   string|   null|\n",
      "|Number of employees|   string|   null|\n",
      "+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Function SQL\n",
    "* OVER and ORDERBY clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train schedule dataset\n",
    "df = spark.read.csv(\"./data/train_schedule.csv\", header=True)\n",
    "df.createOrReplaceTempView(\"sched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------+\n",
      "|train_id|             station|    time|\n",
      "+--------+--------------------+--------+\n",
      "|     110|      Sydney Central|01:05 PM|\n",
      "|     104|        Gare du Nord|01:20 PM|\n",
      "|     107|       Madrid Atocha|01:50 PM|\n",
      "|     115|   Los Angeles Union|01:50 PM|\n",
      "|     102|         Kings Cross|02:00 PM|\n",
      "|     113|        Cairo Ramses|02:35 PM|\n",
      "|     110|   Singapore Central|03:20 PM|\n",
      "|     105|       Tokyo Station|03:35 PM|\n",
      "|     108| Berlin Hauptbahnhof|04:05 PM|\n",
      "|     102|    Shinjuku Station|04:20 PM|\n",
      "|     113| Buenos Aires Retiro|04:50 PM|\n",
      "|     111|Mexico City Terminal|05:35 PM|\n",
      "|     105|   Hong Kong Station|05:50 PM|\n",
      "|     108|  Amsterdam Centraal|06:20 PM|\n",
      "|     103|            Waterloo|06:35 PM|\n",
      "|     106|    Liverpool Street|07:05 AM|\n",
      "|     114|Toronto Union Sta...|07:05 AM|\n",
      "|     111|     Moscow Kazansky|07:50 AM|\n",
      "|     101|       Grand Central|08:15 AM|\n",
      "|     109|        Rome Termini|08:35 AM|\n",
      "+--------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM sched ORDER BY time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/25 11:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/25 11:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/25 11:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/25 11:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/25 11:23:53 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "+--------+-------------+--------+---------+\n",
      "|train_id|      station|    time|time_next|\n",
      "+--------+-------------+--------+---------+\n",
      "|     101|Grand Central|08:15 AM| 10:30 AM|\n",
      "|     101| Penn Station|10:30 AM| 12:45 PM|\n",
      "|     101|Union Station|12:45 PM|     null|\n",
      "+--------+-------------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "WHERE train_id=101\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+--------+---------+\n",
      "|train_id|            station|    time|time_next|\n",
      "+--------+-------------------+--------+---------+\n",
      "|     101|      Grand Central|08:15 AM| 10:30 AM|\n",
      "|     101|       Penn Station|10:30 AM| 12:45 PM|\n",
      "|     101|      Union Station|12:45 PM|     null|\n",
      "|     102|        Kings Cross|02:00 PM| 04:20 PM|\n",
      "|     102|   Shinjuku Station|04:20 PM|     null|\n",
      "|     103|           Waterloo|06:35 PM| 08:50 AM|\n",
      "|     103|   Victoria Station|08:50 AM|     null|\n",
      "|     104|       Gare du Nord|01:20 PM| 11:05 AM|\n",
      "|     104|       King's Cross|11:05 AM|     null|\n",
      "|     105|      Tokyo Station|03:35 PM| 05:50 PM|\n",
      "|     105|  Hong Kong Station|05:50 PM|     null|\n",
      "|     106|   Liverpool Street|07:05 AM| 09:20 AM|\n",
      "|     106|      Beijing South|09:20 AM|     null|\n",
      "|     107|      Madrid Atocha|01:50 PM| 11:35 AM|\n",
      "|     107|      Seoul Station|11:35 AM|     null|\n",
      "|     108|Berlin Hauptbahnhof|04:05 PM| 06:20 PM|\n",
      "|     108| Amsterdam Centraal|06:20 PM|     null|\n",
      "|     109|       Rome Termini|08:35 AM| 10:50 AM|\n",
      "|     109|Zurich Hauptbahnhof|10:50 AM|     null|\n",
      "|     110|     Sydney Central|01:05 PM| 03:20 PM|\n",
      "+--------+-------------------+--------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the PARTITION BY clause in order to improve performance\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/01/25 11:23:59 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist\n",
      "24/01/25 11:23:59 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist\n",
      "24/01/25 11:23:59 WARN Hive: Failed to register all functions.\n",
      "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1709)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3662)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3714)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3694)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getAllFunctions(Hive.java:3956)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions(Hive.java:249)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce(Hive.java:232)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.<init>(Hive.java:389)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.create(Hive.java:333)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.getInternal(Hive.java:313)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.Hive.get(Hive.java:289)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl$.getHive(HiveClientImpl.scala:1339)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.client(HiveClientImpl.scala:262)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:287)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:278)\n",
      "\tat org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:398)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:223)\n",
      "\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101)\n",
      "\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:223)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150)\n",
      "\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:54)\n",
      "\tat org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$1(HiveSessionStateBuilder.scala:69)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog$lzycompute(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.externalCatalog(SessionCatalog.scala:121)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.databaseExists(SessionCatalog.scala:290)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:218)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableRawMetadata(SessionCatalog.scala:513)\n",
      "\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTableMetadata(SessionCatalog.scala:500)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadTable(V2SessionCatalog.scala:66)\n",
      "\tat org.apache.spark.sql.connector.catalog.CatalogV2Util$.loadTable(CatalogV2Util.scala:311)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1202)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1201)\n",
      "\tat scala.Option.orElse(Option.scala:447)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1193)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1064)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$13.applyOrElse(Analyzer.scala:1028)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1228)\n",
      "\tat org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1227)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1028)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:987)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)\n",
      "\tat scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:91)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:431)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:173)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:227)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:188)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:212)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:211)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.reflect.InvocationTargetException\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480)\n",
      "\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1707)\n",
      "\t... 108 more\n",
      "Caused by: MetaException(message:org/datanucleus/store/query/cache/QueryCompilationCache)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:83)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:92)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:6950)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:164)\n",
      "\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:70)\n",
      "\t... 114 more\n",
      "Caused by: java.lang.NoClassDefFoundError: org/datanucleus/store/query/cache/QueryCompilationCache\n",
      "\tat java.base/java.lang.Class.getDeclaredMethods0(Native Method)\n",
      "\tat java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3402)\n",
      "\tat java.base/java.lang.Class.getMethodsRecursive(Class.java:3543)\n",
      "\tat java.base/java.lang.Class.getMethod0(Class.java:3529)\n",
      "\tat java.base/java.lang.Class.getMethod(Class.java:2225)\n",
      "\tat javax.jdo.JDOHelper$15.run(JDOHelper.java:1944)\n",
      "\tat javax.jdo.JDOHelper$15.run(JDOHelper.java:1942)\n",
      "\tat java.base/java.security.AccessController.doPrivileged(AccessController.java:569)\n",
      "\tat javax.jdo.JDOHelper.getMethod(JDOHelper.java:1941)\n",
      "\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1163)\n",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n",
      "\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:521)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:550)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper(ObjectStore.java:405)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:342)\n",
      "\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:303)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:79)\n",
      "\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:140)\n",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:58)\n",
      "\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:67)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStoreForConf(HiveMetaStore.java:629)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMSForConf(HiveMetaStore.java:595)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:589)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:656)\n",
      "\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:432)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:148)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\n",
      "\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:79)\n",
      "\t... 118 more\n",
      "Caused by: java.lang.ClassNotFoundException: org.datanucleus.store.query.cache.QueryCompilationCache\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:641)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\t... 151 more\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 22\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Updated query -> Query did not include PARTITION BY clause as well as bad_row number\u001b[39;00m\n\u001b[1;32m     13\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124mSELECT \u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124mROW_NUMBER() OVER (ORDER BY time) AS row,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mFROM schedule\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Give the number of the bad row as an integer\u001b[39;00m\n\u001b[1;32m     25\u001b[0m bad_row \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/session.py:1034\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     sqlQuery \u001b[38;5;241m=\u001b[39m formatter\u001b[38;5;241m.\u001b[39mformat(sqlQuery, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "#The following are example queries\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Updated query -> Query did not include PARTITION BY clause as well as bad_row number\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "# Give the number of the bad row as an integer\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause, SQL keywords in upper case\n",
    "clause = 'PARTITION BY train_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train_id', 'station', 'time']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ways to select 2 columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+--------+\n",
      "|train_id|         station|    time|\n",
      "+--------+----------------+--------+\n",
      "|     101|   Grand Central|08:15 AM|\n",
      "|     101|    Penn Station|10:30 AM|\n",
      "|     101|   Union Station|12:45 PM|\n",
      "|     102|     Kings Cross|02:00 PM|\n",
      "|     102|Shinjuku Station|04:20 PM|\n",
      "+--------+----------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|train_id|         station|\n",
      "+--------+----------------+\n",
      "|     101|   Grand Central|\n",
      "|     101|    Penn Station|\n",
      "|     101|   Union Station|\n",
      "|     102|     Kings Cross|\n",
      "|     102|Shinjuku Station|\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show only 5 records and 2 columns\n",
    "df.select('train_id', 'station').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|train_id|         station|\n",
      "+--------+----------------+\n",
      "|     101|   Grand Central|\n",
      "|     101|    Penn Station|\n",
      "|     101|   Union Station|\n",
      "|     102|     Kings Cross|\n",
      "|     102|Shinjuku Station|\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The same can be achieved using <dot > notation\n",
    "df.select(df.train_id, df.station).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The <col> function can also be imported\n",
    "# This enables passing in column names as strings\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|train_id|         station|\n",
      "+--------+----------------+\n",
      "|     101|   Grand Central|\n",
      "|     101|    Penn Station|\n",
      "|     101|   Union Station|\n",
      "|     102|     Kings Cross|\n",
      "|     102|Shinjuku Station|\n",
      "+--------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(col('train_id'), col('station')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|train|         station|\n",
      "+-----+----------------+\n",
      "|  101|   Grand Central|\n",
      "|  101|    Penn Station|\n",
      "|  101|   Union Station|\n",
      "|  102|     Kings Cross|\n",
      "|  102|Shinjuku Station|\n",
      "+-----+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the <withColumnRenamed> function\n",
    "df.select('train_id', 'station').withColumnRenamed('train_id', 'train').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note -> Avoid using all 3 conventions at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|train|         station|\n",
      "+-----+----------------+\n",
      "|  101|   Grand Central|\n",
      "|  101|    Penn Station|\n",
      "|  101|   Union Station|\n",
      "|  102|     Kings Cross|\n",
      "|  102|Shinjuku Station|\n",
      "+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = 'sched'\n",
    "target_col = 'train_id'\n",
    "sup_col = 'station'\n",
    "\n",
    "spark.sql(f'SELECT {target_col} AS train, {sup_col} FROM {table_name} LIMIT 5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------+\n",
      "|train|         station|\n",
      "+-----+----------------+\n",
      "|  101|   Grand Central|\n",
      "|  101|    Penn Station|\n",
      "|  101|   Union Station|\n",
      "|  102|     Kings Cross|\n",
      "|  102|Shinjuku Station|\n",
      "+-----+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using dot notation to achieve the same results\n",
    "df.select(col('train_id').alias('train'), 'station').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------+---+\n",
      "|train_id|          station|    time| id|\n",
      "+--------+-----------------+--------+---+\n",
      "|     101|    Grand Central|08:15 AM|  1|\n",
      "|     101|     Penn Station|10:30 AM|  2|\n",
      "|     101|    Union Station|12:45 PM|  3|\n",
      "|     102|      Kings Cross|02:00 PM|  1|\n",
      "|     102| Shinjuku Station|04:20 PM|  2|\n",
      "|     103|         Waterloo|06:35 PM|  1|\n",
      "|     103| Victoria Station|08:50 AM|  2|\n",
      "|     104|     Gare du Nord|01:20 PM|  1|\n",
      "|     104|     King's Cross|11:05 AM|  2|\n",
      "|     105|    Tokyo Station|03:35 PM|  1|\n",
      "|     105|Hong Kong Station|05:50 PM|  2|\n",
      "+--------+-----------------+--------+---+\n",
      "only showing top 11 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Window Functions to achieve the same results\n",
    "\n",
    "# The following query adds a number to each stop on a train line -- in a new column called id \n",
    "query = \"\"\"\n",
    "SELECT *,\n",
    "ROW_NUMBER() OVER(PARTITION BY train_id ORDER BY time) AS id\n",
    "FROM sched\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------+---+\n",
      "|train_id|          station|    time| id|\n",
      "+--------+-----------------+--------+---+\n",
      "|     101|    Grand Central|08:15 AM|  1|\n",
      "|     101|     Penn Station|10:30 AM|  2|\n",
      "|     101|    Union Station|12:45 PM|  3|\n",
      "|     102|      Kings Cross|02:00 PM|  1|\n",
      "|     102| Shinjuku Station|04:20 PM|  2|\n",
      "|     103|         Waterloo|06:35 PM|  1|\n",
      "|     103| Victoria Station|08:50 AM|  2|\n",
      "|     104|     Gare du Nord|01:20 PM|  1|\n",
      "|     104|     King's Cross|11:05 AM|  2|\n",
      "|     105|    Tokyo Station|03:35 PM|  1|\n",
      "|     105|Hong Kong Station|05:50 PM|  2|\n",
      "+--------+-----------------+--------+---+\n",
      "only showing top 11 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Window Functions to achieve the same results with dot notation\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "df.withColumn(\"id\", row_number().over(Window.partitionBy('train_id').orderBy('time'))).show(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|train_id|   start|\n",
      "+--------+--------+\n",
      "|     101|08:15 AM|\n",
      "|     102|02:00 PM|\n",
      "|     103|06:35 PM|\n",
      "|     104|01:20 PM|\n",
      "|     105|03:35 PM|\n",
      "|     106|07:05 AM|\n",
      "|     107|01:50 PM|\n",
      "|     108|04:05 PM|\n",
      "|     109|08:35 AM|\n",
      "|     110|01:05 PM|\n",
      "|     111|05:35 PM|\n",
      "|     112|10:05 AM|\n",
      "|     113|02:35 PM|\n",
      "|     114|07:05 AM|\n",
      "|     115|01:50 PM|\n",
      "+--------+--------+\n",
      "\n",
      "+--------+--------+\n",
      "|train_id|   start|\n",
      "+--------+--------+\n",
      "|     101|08:15 AM|\n",
      "|     102|02:00 PM|\n",
      "|     103|06:35 PM|\n",
      "|     104|01:20 PM|\n",
      "|     105|03:35 PM|\n",
      "|     106|07:05 AM|\n",
      "|     107|01:50 PM|\n",
      "|     108|04:05 PM|\n",
      "|     109|08:35 AM|\n",
      "|     110|01:05 PM|\n",
      "|     111|05:35 PM|\n",
      "|     112|10:05 AM|\n",
      "|     113|02:35 PM|\n",
      "|     114|07:05 AM|\n",
      "|     115|01:50 PM|\n",
      "+--------+--------+\n",
      "\n",
      "+--------+---------+---------+\n",
      "|train_id|min(time)|max(time)|\n",
      "+--------+---------+---------+\n",
      "|     101| 08:15 AM| 12:45 PM|\n",
      "|     102| 02:00 PM| 04:20 PM|\n",
      "|     103| 06:35 PM| 08:50 AM|\n",
      "|     104| 01:20 PM| 11:05 AM|\n",
      "|     105| 03:35 PM| 05:50 PM|\n",
      "|     106| 07:05 AM| 09:20 AM|\n",
      "|     107| 01:50 PM| 11:35 AM|\n",
      "|     108| 04:05 PM| 06:20 PM|\n",
      "|     109| 08:35 AM| 10:50 AM|\n",
      "|     110| 01:05 PM| 03:20 PM|\n",
      "|     111| 05:35 PM| 07:50 AM|\n",
      "|     112| 10:05 AM| 12:20 PM|\n",
      "|     113| 02:35 PM| 04:50 PM|\n",
      "|     114| 07:05 AM| 09:20 AM|\n",
      "|     115| 01:50 PM| 11:35 AM|\n",
      "+--------+---------+---------+\n",
      "\n",
      "+--------+---------+\n",
      "|train_id|max(time)|\n",
      "+--------+---------+\n",
      "|     101| 12:45 PM|\n",
      "|     102| 04:20 PM|\n",
      "|     103| 08:50 AM|\n",
      "|     104| 11:05 AM|\n",
      "|     105| 05:50 PM|\n",
      "|     106| 09:20 AM|\n",
      "|     107| 11:35 AM|\n",
      "|     108| 06:20 PM|\n",
      "|     109| 10:50 AM|\n",
      "|     110| 03:20 PM|\n",
      "|     111| 07:50 AM|\n",
      "|     112| 12:20 PM|\n",
      "|     113| 04:50 PM|\n",
      "|     114| 09:20 AM|\n",
      "|     115| 11:35 AM|\n",
      "+--------+---------+\n",
      "\n",
      "max(time)\n"
     ]
    }
   ],
   "source": [
    "# Consider the following examples\n",
    "# Give the identical result in each command\n",
    "spark.sql('SELECT train_id, MIN(time) AS start FROM sched GROUP BY train_id').show()\n",
    "df.groupBy('train_id').agg({'time':'min'}).withColumnRenamed('min(time)', 'start').show()\n",
    "\n",
    "# Print the second column of the result\n",
    "spark.sql('SELECT train_id, MIN(time), MAX(time) FROM sched GROUP BY train_id').show()\n",
    "result = df.groupBy('train_id').agg({'time':'min', 'time':'max'})\n",
    "result.show()\n",
    "print(result.columns[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+\n",
      "|train_id|   start|     end|\n",
      "+--------+--------+--------+\n",
      "|     101|08:15 AM|12:45 PM|\n",
      "|     102|02:00 PM|04:20 PM|\n",
      "|     103|06:35 PM|08:50 AM|\n",
      "|     104|01:20 PM|11:05 AM|\n",
      "|     105|03:35 PM|05:50 PM|\n",
      "+--------+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregating the same column twice\n",
    "\n",
    "# There are cases where dot notation can be more cumbersome than SQL. \n",
    "# This sample code calculates the first and last times for each train line. \n",
    "# The following code does this using dot notation.\n",
    "\n",
    "from pyspark.sql.functions import min, max, col\n",
    "expr = [min(col(\"time\")).alias('start'), max(col(\"time\")).alias('end')]\n",
    "dot_df = df.groupBy(\"train_id\").agg(*expr)\n",
    "dot_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Local)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
