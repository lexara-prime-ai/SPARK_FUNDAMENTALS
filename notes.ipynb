{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark fundamentals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pyspark(Spark's python wrapper)\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "file_path = \"./data/people.csv\"\n",
    "\n",
    "# Load data from a csv file\n",
    "df = spark.read.csv(file_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define table name\n",
    "table_name = \"organizations\"\n",
    "\n",
    "# Create an SQL table \n",
    "df.createOrReplaceTempView(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define query\n",
    "query = f\"SELECT * FROM {table_name}\"\n",
    "\n",
    "# Query the table\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SHOW COLUMNS FROM {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"SELECT * FROM {table_name} LIMIT 0\")\n",
    "\n",
    "print(result.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect table schema\n",
    "result = spark.sql(f\"DESCRIBE {table_name}\")\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window Function SQL\n",
    "* OVER and ORDERBY clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train schedule dataset\n",
    "df = spark.read.csv(\"./data/train_schedule.csv\", header=True)\n",
    "df.createOrReplaceTempView(\"sched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM sched ORDER BY time\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "WHERE train_id=101\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the PARTITION BY clause in order to improve performance\n",
    "query = \"\"\"\n",
    "SELECT train_id, station, time,\n",
    "LEAD(time, 1) OVER (PARTITION BY train_id ORDER BY time) AS time_next\n",
    "FROM sched\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following are example queries\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "\n",
    "# Updated query -> Query did not include PARTITION BY clause as well as bad_row number\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "ROW_NUMBER() OVER (ORDER BY time) AS row,\n",
    "train_id, \n",
    "station, \n",
    "time, \n",
    "LEAD(time,1) OVER (PARTITION BY train_id ORDER BY time) AS time_next \n",
    "FROM schedule\n",
    "\"\"\"\n",
    "spark.sql(query).show()\n",
    "\n",
    "# Give the number of the bad row as an integer\n",
    "bad_row = 7\n",
    "\n",
    "# Provide the missing clause, SQL keywords in upper case\n",
    "clause = 'PARTITION BY train_id'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
